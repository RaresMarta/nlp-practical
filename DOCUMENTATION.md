# Bible Semantic Similarity - Complete Documentation

## Table of Contents
1. [Project Overview](#project-overview)
2. [Dataset](#dataset)
3. [Training Process](#training-process)
4. [Model Architecture](#model-architecture)
5. [Application Architecture](#application-architecture)
6. [How the Application Works](#how-the-application-works)
7. [Installation and Setup](#installation-and-setup)
8. [Usage Guide](#usage-guide)
9. [API Reference](#api-reference)
10. [Technical Details](#technical-details)

---

## Project Overview

**Bible Semantic Similarity Explorer** is a web-based application that leverages advanced NLP techniques (Word2Vec and Doc2Vec) to explore semantic relationships in Romanian Bible text. The application allows users to:

- Find semantically similar words
- Compute word pair similarities
- Discover similar Bible passages
- Explore word analogies

The project demonstrates practical applications of embedding-based NLP models in a user-friendly Gradio interface.

**Language:** Romanian
**Dataset:** Romanian Bible Paraphrase Dataset (~247,000 training pairs)
**Models Used:** Word2Vec (word embeddings) and Doc2Vec (paragraph embeddings)

---

## Dataset

### Data Source
- **Name:** Romanian Bible Paraphrase Dataset (HuggingFace)
- **Source:** `andyP/ro-paraphrase-bible`
- **Training Pairs:** ~247,000
- **Content:** Romanian Bible paraphrases (translations and variations)

### Data Statistics
| Metric | Value |
|--------|-------|
| Total Training Pairs | ~247,000 |
| Unique Passages | ~247,000+ (after deduplication) |
| Language | Romanian |
| Text Type | Biblical text and paraphrases |

### Data Loading Process
The application loads the dataset dynamically using the HuggingFace `datasets` library:

```python
from datasets import load_dataset

dataset = load_dataset("andyP/ro-paraphrase-bible")
text_columns = [col for col in dataset['train'].column_names
                if dataset['train'].features[col].dtype == 'string']
col1, col2 = text_columns[0], text_columns[1]

# Extract all unique passages
list1 = [s for s in dataset['train'][col1] if s is not None]
list2 = [s for s in dataset['train'][col2] if s is not None]
original_sentences_list = list(set(list1 + list2))
```

**Key Characteristics:**
- Two text columns representing paraphrased Bible passages
- Handled None values by filtering
- Deduplicated sentences using set operations
- Preserved original text for retrieval

---

## Training Process

### Step 1: Data Preprocessing

#### Text Normalization
The training process begins with text preprocessing using spaCy's Romanian language model:

```python
import spacy
from tqdm.auto import tqdm

nlp = spacy.load("ro_core_news_sm", disable=['ner', 'parser'])
```

**Preprocessing Pipeline:**
1. **Tokenization:** Split text into individual tokens (words)
2. **Lemmatization:** Convert words to their base form (e.g., "iubind" â†’ "iubi")
3. **Lowercasing:** Convert all text to lowercase for consistency
4. **Punctuation & Whitespace Removal:** Filter out punctuation marks and whitespace

**Example:**
```
Input: "Dumnezeu este iubire infinitÄƒ!"
Output: ["dumnezeu", "fi", "iubire", "infinit"]
```

#### Parallel Processing
To speed up preprocessing of 247,000+ sentences, spaCy's parallel processing was used:

```python
processed_corpus = []

for doc in tqdm(nlp.pipe(original_sentences_list,
                         batch_size=3000,
                         n_process=-1),
                total=len(original_sentences_list),
                desc="Parallel Tokenizing"):
    tokens = [token.lemma_.lower() for token in doc
              if not token.is_punct and not token.is_space]
    processed_corpus.append(tokens)
```

**Parameters:**
- `batch_size=3000:` Process 3,000 sentences per batch
- `n_process=-1:` Use all available CPU cores

### Step 2: Word2Vec Model Training

#### Architecture
**Word2Vec (CBOW - Continuous Bag of Words)**

```python
from gensim.models import Word2Vec

w2v_model = Word2Vec(
    sentences=processed_corpus,
    vector_size=100,      # Dimensionality of word vectors
    window=5,             # Context window size (5 words on each side)
    min_count=2,          # Minimum word frequency threshold
    workers=4             # Number of parallel threads
)

w2v_model.train(processed_corpus,
                total_examples=len(processed_corpus),
                epochs=10)

w2v_model.save("bible_ro_word2vec.model")
```

#### How CBOW Works
The Continuous Bag of Words (CBOW) algorithm predicts a target word from its surrounding context words:

```
Context: [word_-2, word_-1, word_0, word_1, word_2]
                                    â†‘
                            Predicted word
```

**Training Objective:** Minimize the error in predicting the center word given the surrounding context.

#### Model Parameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **vector_size** | 100 | Dimensionality of word embeddings (100-dimensional space) |
| **window** | 5 | Context window size (words within 5 positions on each side) |
| **min_count** | 2 | Minimum frequency threshold (words appearing < 2 times are ignored) |
| **workers** | 4 | Number of threads for parallel processing |
| **epochs** | 10 | Number of passes through the training data |

#### Output
- **Vocabulary Size:** Approximately 10,000+ unique lemmas
- **Vector Dimension:** 100-dimensional dense vectors
- **File:** `bible_ro_word2vec.model` (~34 MB)

### Step 3: Doc2Vec Model Training

#### Architecture
**Doc2Vec (PV-DM - Paragraph Vector Distributed Memory)**

```python
from gensim.models.doc2vec import Doc2Vec, TaggedDocument

# Tag each document with its index
tagged_data = [TaggedDocument(words=doc, tags=[str(i)])
               for i, doc in enumerate(processed_corpus)]

d2v_model = Doc2Vec(
    vector_size=100,      # Dimensionality of document vectors
    window=5,             # Context window size
    min_count=2,          # Minimum word frequency
    dm=1,                 # 1 = Distributed Memory (PV-DM), 0 = DBOW
    epochs=20,            # Number of training epochs
    workers=4             # Parallel threads
)

d2v_model.build_vocab(tagged_data)
d2v_model.train(tagged_data,
                total_examples=d2v_model.corpus_count,
                epochs=d2v_model.epochs)

d2v_model.save("bible_ro_doc2vec.model")
```

#### How PV-DM Works
Paragraph Vector Distributed Memory treats each document as a "word" in a larger context:

```
Document Vector (from model) + Context Words â†’ Predict Target Word
```

**Training Objective:** Learn document-level representations that capture semantic meaning of entire passages.

#### Model Parameters

| Parameter | Value | Purpose |
|-----------|-------|---------|
| **vector_size** | 100 | Dimensionality of document embeddings |
| **window** | 5 | Context window within the document |
| **min_count** | 2 | Minimum word frequency threshold |
| **dm** | 1 | PV-DM mode (distributed memory) |
| **epochs** | 20 | Number of training passes (higher for document-level learning) |
| **workers** | 4 | Parallel processing threads |

#### Output
- **Document Count:** ~247,000+ unique passages
- **Vector Dimension:** 100-dimensional dense vectors
- **File:** `bible_ro_doc2vec.model` (~24 MB)

### Training Timeline Summary

```
1. Data Loading & Cleaning
   â””â”€> Deduplicate passages (~247,000 unique)

2. Preprocessing (Parallel)
   â””â”€> Tokenize + Lemmatize + Lowercase
   â””â”€> Remove punctuation/whitespace
   â””â”€> Time: ~5-10 minutes (4 CPU cores)

3. Word2Vec Training
   â””â”€> CBOW algorithm, 100 dims, 10 epochs
   â””â”€> Learns ~10,000+ word embeddings
   â””â”€> Time: ~2-5 minutes

4. Doc2Vec Training
   â””â”€> PV-DM algorithm, 100 dims, 20 epochs
   â””â”€> Learns ~247,000 document embeddings
   â””â”€> Time: ~10-15 minutes

Total Training Time: ~20-30 minutes (on 4-core machine)
```

---

## Model Architecture

### Word2Vec Model

```
Input Layer (Context Words)
    â†“
Embedding Lookup [2 Ã— window Ã— vector_size = 200]
    â†“
Hidden Layer (Average)
    â†“
Output Layer (Softmax)
    â†“
Predicted Word Probability
```

**Characteristics:**
- **Input:** 5-word context window (2 words on each side)
- **Embedding Dimension:** 100-dimensional vectors
- **Vocabulary:** 10,000+ unique words
- **Output:** Word probability distribution

**Semantic Properties:**
- Similar words have similar vectors
- Vector arithmetic: king - man + woman â‰ˆ queen
- Cosine similarity measures word relationship

### Doc2Vec Model

```
Document Vector (from model) + Context Words
    â†“
Embedding Lookup & Concatenation
    â†“
Hidden Layer
    â†“
Output Layer (Softmax)
    â†“
Predicted Word Probability
```

**Characteristics:**
- **Document Representation:** Unique vector for each passage
- **Context Integration:** Fixed-size representation of variable-length text
- **Dimension:** 100-dimensional vectors
- **Training Method:** Distributed Memory (PV-DM)

**Semantic Properties:**
- Document vectors capture overall semantic meaning
- Similar passages have similar vectors
- Inference allows generating vectors for new texts

---

## Application Architecture

### Folder Structure
```
nlp-practical/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py           # Package initialization
â”‚   â”œâ”€â”€ models.py             # Model loading
â”‚   â”œâ”€â”€ preprocessing.py      # Text preprocessing
â”‚   â”œâ”€â”€ services.py           # NLP services and analysis
â”‚   â””â”€â”€ ui.py                 # Gradio interface
â”œâ”€â”€ app.py                    # Entry point
â”œâ”€â”€ bible_ro_word2vec.model   # Trained Word2Vec model
â”œâ”€â”€ bible_ro_doc2vec.model    # Trained Doc2Vec model
â”œâ”€â”€ Project.ipynb             # Training notebook
â””â”€â”€ DOCUMENTATION.md          # This file
```

### Component Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Gradio Web Interface (ui.py)             â”‚
â”‚  - Similar Words Tab                            â”‚
â”‚  - Word Pair Similarity Tab                     â”‚
â”‚  - Word Analogy Tab                            â”‚
â”‚  - Sentence Similarity Tab                      â”‚
â”‚  - Model Information Panel                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ services.py â”‚   â”‚ models.py      â”‚
   â”‚             â”‚   â”‚                â”‚
   â”‚ - Find      â”‚   â”‚ - Load Word2Vecâ”‚
   â”‚   Similar   â”‚   â”‚ - Load Doc2Vec â”‚
   â”‚   Words     â”‚   â”‚ - Load spaCy   â”‚
   â”‚ - Pair      â”‚   â”‚                â”‚
   â”‚   Similarityâ”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚ - Analogy   â”‚
   â”‚ - Sentences â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚preprocessing.py        â”‚
   â”‚                        â”‚
   â”‚- Tokenization (spaCy)  â”‚
   â”‚- Lemmatization        â”‚
   â”‚- Lowercase            â”‚
   â”‚- Punctuation removal  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Pre-trained Models            â”‚
   â”‚  - Word2Vec (100-dim)          â”‚
   â”‚  - Doc2Vec (100-dim)           â”‚
   â”‚  - spaCy ro_core_news_sm       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

```
User Input (Text in UI)
    â†“
[preprocessing.py] â†’ Tokenize + Lemmatize
    â†“
[services.py] â†’ Query Word2Vec/Doc2Vec Models
    â†“
[services.py] â†’ Compute Similarity Scores
    â†“
[services.py] â†’ Format Results with Statistics
    â†“
[ui.py] â†’ Display in Markdown
    â†“
User (Sees Results)
```

---

## How the Application Works

### Module: `models.py`

**Purpose:** Load and initialize all pre-trained models.

```python
# Load spaCy Romanian language model
nlp = spacy.load("ro_core_news_sm", disable=['ner', 'parser'])

# Load Word2Vec model
w2v_model = Word2Vec.load("bible_ro_word2vec.model")

# Load Doc2Vec model
d2v_model = Doc2Vec.load("bible_ro_doc2vec.model")
```

**Output:**
- spaCy model for tokenization and lemmatization
- Word2Vec model with 10,000+ word vectors
- Doc2Vec model with 247,000+ document vectors

### Module: `preprocessing.py`

**Purpose:** Preprocess user input text.

```python
def preprocess(text: str) -> list[str]:
    """Preprocess text by tokenizing and lemmatizing."""
    doc = nlp(str(text))
    return [token.lemma_.lower() for token in doc
            if not token.is_punct and not token.is_space]
```

**Process:**
1. Parse text with spaCy
2. Extract lemmas (base forms)
3. Convert to lowercase
4. Filter punctuation and whitespace

**Example:**
```
Input: "iubiri frumoase È™i adeverate"
Output: ["iubire", "frumos", "È™i", "adevÄƒrat"]
```

### Module: `services.py`

**Purpose:** Implement NLP operations using trained models.

#### 1. **find_similar_words(word, top_n)**

**Algorithm:**
```
1. Preprocess input word â†’ get lemma
2. Check if lemma exists in Word2Vec vocabulary
3. Use Word2Vec.wv.most_similar() to find similar words
4. Compute vector statistics for the query word
5. Return ranked list with similarity scores
```

**Example:**
```
Input: "iubire" (love)
Output:
  1. dragoste (0.8234) - love/affection
  2. chemare (0.7892) - calling/summons
  3. devoÈ›iune (0.7456) - devotion
```

**Similarity Metric:** Cosine similarity (0-1 scale)

#### 2. **compute_word_pair_similarity(word1, word2)**

**Algorithm:**
```
1. Preprocess both words â†’ get lemmas
2. Check both words in vocabulary
3. Compute cosine similarity using Word2Vec vectors
4. Calculate additional metrics:
   - Vector norms (L2 magnitude)
   - Dot product
   - Cosine distance
5. Interpret similarity (Very High > High > Moderate > Low > Very Low)
6. Return formatted comparison
```

**Example:**
```
Input: "Dumnezeu" (God) vs "Domnul" (Lord)
Similarity: 0.8723 (Very High Similarity)
Interpretation: These words are nearly synonymous
```

#### 3. **find_similar_sentences(sentence, top_n)**

**Algorithm:**
```
1. Preprocess input sentence â†’ get token list
2. Infer document vector using Doc2Vec model
   Doc2Vec.infer_vector(tokens)
3. Find most similar documents using cosine similarity
   Doc2Vec.dv.most_similar([inferred_vector], topn=top_n)
4. Retrieve original Bible passages for results
5. Compute vector statistics
6. Return ranked passages with similarity scores
```

**Example:**
```
Input: "Dumnezeu este iubire"
Output:
  1. "Dumnezeu iubeÈ™te lumea..." (0.7234)
  2. "Iubirea lui Dumnezeu..." (0.6892)
  3. "Cine iubeÈ™te, cunoaÈ™te Dumnezeu..." (0.6456)
```

#### 4. **word_analogy(word_a, word_b, word_c, top_n)**

**Algorithm:**
```
1. Preprocess three words â†’ get lemmas
2. Verify all words in vocabulary
3. Compute analogy vector: A - B + C
4. Find words closest to analogy vector
   Word2Vec.wv.most_similar(positive=[A, C], negative=[B])
5. Return ranked analogies
```

**Example:**
```
Analogy: "tatÄƒl" is to "fiul" as "mama" is to ?
Output:
  1. fiica (daughter) - 0.8934
  2. mamÄƒ (mother) - 0.7234
  3. pÄƒrinÈ›i (parents) - 0.6789
```

**Mathematical Foundation:**
$$\text{Analogy Vector} = \vec{A} - \vec{B} + \vec{C}$$
$$\text{Result} = \arg\max_w \cos(\vec{w}, \vec{\text{Analogy}})$$

### Module: `ui.py`

**Purpose:** Create the web interface using Gradio.

```python
def create_demo() -> gr.Blocks:
    # Create theme
    theme = gr.themes.Soft(primary_hue="indigo", secondary_hue="blue")

    # Create interface blocks
    with gr.Blocks(title="Bible Semantic Similarity") as demo:
        # Display title and description
        # Create accordion with model statistics
        # Create tabs for different operations
        # Bind callbacks to buttons

    return demo
```

**Interface Tabs:**
1. **Similar Words** - Find semantically similar words
2. **Word Pair Similarity** - Compare two words
3. **Word Analogy** - Solve word relationships
4. **Sentence Similarity** - Find similar Bible passages
5. **Model Information** - Display dataset & model statistics

### Module: `app.py`

**Purpose:** Application entry point.

```python
from app.ui import create_demo

if __name__ == "__main__":
    demo = create_demo()
    demo.launch()
```

**Launches Gradio server:**
- Default: `http://localhost:7860`
- Provides web-based interface for all operations

---

## Installation and Setup

### Prerequisites
- Python 3.8+
- 2-3 GB free disk space (for models)
- 4+ GB RAM
- macOS/Linux/Windows

### Step 1: Clone/Download Project
```bash
cd /path/to/nlp-practical
```

### Step 2: Create Virtual Environment
```bash
# Using Conda
conda create -n nlp python=3.10
conda activate nlp

# Or using venv
python -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

**Required Packages:**
```
gradio>=6.0
gensim>=4.2.0
spacy>=3.5.0
datasets>=2.10.0
numpy>=1.21.0
```

### Step 4: Download spaCy Model
```bash
python -m spacy download ro_core_news_sm
```

### Step 5: Verify Models are Present
```bash
ls -lh *.model
# Should see:
# bible_ro_word2vec.model
# bible_ro_doc2vec.model
```

### Step 6: Run Application
```bash
python app.py
# or
gradio app.py
```

**Output:**
```
Running on local URL:  http://127.0.0.1:7860
Running on public URL: https://xyz.gradio.live
```

---

## Usage Guide

### 1. Finding Similar Words

**Use Case:** Explore words with similar meanings

**Steps:**
1. Click **"ðŸ”¤ Similar Words"** tab
2. Enter a Romanian word (e.g., "Dumnezeu", "iubire", "credinÈ›Äƒ")
3. Adjust "Number of results" slider (default: 10)
4. Click **"Find Similar Words"**

**Output:**
- Lemmatized form of query word
- Word vector statistics (norm, mean, std)
- Table with top N similar words:
  - Rank
  - Similar word
  - Cosine similarity score (0-1)
  - Visual bar representation
  - Vector norm of each result

**Example Query:** "Dumnezeu"
```
Word Vector Statistics
- Vector Dimension: 100
- Vector Norm: 2.3456
- Mean Value: 0.0123
- Std Deviation: 0.8456

Top Similar Words
| Rank | Similar Word | Similarity | Vector Norm |
|------|--------------|------------|-------------|
| 1    | Domnul       | 0.8723     | 2.4567      |
| 2    | Spiritul     | 0.7892     | 2.2345      |
| 3    | Creator      | 0.7234     | 2.1234      |
```

### 2. Comparing Two Words

**Use Case:** Understand relationship between specific words

**Steps:**
1. Click **"âš–ï¸ Word Pair Similarity"** tab
2. Enter first word (e.g., "Dumnezeu")
3. Enter second word (e.g., "Domnul")
4. Click **"Compare Words"**

**Output:**
- Similarity metrics:
  - Cosine similarity (0-1 scale)
  - Cosine distance (1 - similarity)
  - Dot product
  - Vector norms
- Visual similarity bar
- Interpretation (Very High/High/Moderate/Low/Very Low)

**Similarity Interpretation:**
- **> 0.8:** Nearly synonymous
- **0.6-0.8:** Closely related
- **0.4-0.6:** Moderate relationship
- **0.2-0.4:** Loosely related
- **< 0.2:** Little semantic connection

### 3. Word Analogies

**Use Case:** Solve word relationships (A is to B as C is to ?)

**Steps:**
1. Click **"ðŸ§® Word Analogy"** tab
2. Enter Word A (e.g., "tatÄƒl")
3. Enter Word B (e.g., "fiul")
4. Enter Word C (e.g., "mama")
5. Adjust "Number of results" slider
6. Click **"Find Analogy"**

**Output:**
- Analogy equation: "A is to B as C is to ?"
- Analogy vector statistics
- Top N word suggestions with scores

**Example:**
```
"tatÄƒl" is to "fiul" as "mama" is to...
1. fiica (0.8934) - daughter
2. mamÄƒ (0.7234) - mother
3. pÄƒrinÈ›i (0.6789) - parents
```

### 4. Finding Similar Sentences

**Use Case:** Discover related Bible passages

**Steps:**
1. Click **"ðŸ“ Sentence Similarity"** tab
2. Enter a sentence in Romanian
3. Adjust "Number of results" slider
4. Click **"Find Similar Passages"**

**Output:**
- Query vector statistics (tokens, norm, mean, std)
- Table with similar passages:
  - Rank
  - Similarity score
  - Document vector norm
  - Actual Bible passage text
- Dataset statistics

**Example Query:** "Dumnezeu este iubire"
```
Query Vector Statistics
- Tokens: 4
- Vector Dimension: 100
- Vector Norm: 1.2345
- Mean Value: 0.0456
- Std Deviation: 0.7234

Most Similar Bible Passages
| Rank | Similarity | Doc Norm | Passage |
|------|------------|----------|---------|
| 1    | 0.7234     | 1.1234   | Dumnezeu... |
| 2    | 0.6892     | 1.0987   | Iubirea... |
| 3    | 0.6456     | 1.0234   | Cine... |
```

### 5. Viewing Model Information

**Use Case:** Understand model architecture and dataset

**Steps:**
1. Click **"ðŸ“Š Dataset & Model Information"** accordion
2. View displayed statistics:
   - Dataset size
   - Word2Vec parameters
   - Doc2Vec parameters

**Displayed Information:**
```
Dataset Statistics
- Total Unique Passages: 247,000+
- Training Pairs: 247,000

Word2Vec Model
- Vocabulary Size: 10,000+ words
- Vector Dimension: 100
- Window Size: 5 words
- Min Count: 2
- Training Epochs: 10
- Training Method: CBOW

Doc2Vec Model
- Total Documents: 247,000+
- Vector Dimension: 100
- Window Size: 5 words
- Min Count: 2
- Training Epochs: 20
- Training Method: Paragraph Vectors
```

---

## API Reference

### Core Functions

#### `preprocess(text: str) -> list[str]`

Preprocesses input text using spaCy Romanian model.

**Parameters:**
- `text` (str): Input text in Romanian

**Returns:**
- `list[str]`: List of lemmatized, lowercased tokens

**Example:**
```python
from app.preprocessing import preprocess

tokens = preprocess("Iubirea lui Dumnezeu")
# Output: ['iubire', 'al', 'dumnezeu']
```

---

#### `find_similar_words(word: str, top_n: int = 10) -> str`

Finds semantically similar words using Word2Vec.

**Parameters:**
- `word` (str): Romanian word to search
- `top_n` (int): Number of similar words to return (1-20)

**Returns:**
- `str`: Formatted markdown with results and statistics

**Example:**
```python
from app.services import find_similar_words

result = find_similar_words("iubire", top_n=5)
print(result)
```

---

#### `compute_word_pair_similarity(word1: str, word2: str) -> str`

Computes similarity between two words.

**Parameters:**
- `word1` (str): First Romanian word
- `word2` (str): Second Romanian word

**Returns:**
- `str`: Formatted markdown with similarity metrics

**Example:**
```python
from app.services import compute_word_pair_similarity

result = compute_word_pair_similarity("Dumnezeu", "Domnul")
print(result)
```

---

#### `find_similar_sentences(sentence: str, top_n: int = 10) -> str`

Finds similar Bible passages using Doc2Vec.

**Parameters:**
- `sentence` (str): Query sentence in Romanian
- `top_n` (int): Number of similar passages (1-20)

**Returns:**
- `str`: Formatted markdown with similar passages

**Example:**
```python
from app.services import find_similar_sentences

result = find_similar_sentences("Dumnezeu este iubire", top_n=5)
print(result)
```

---

#### `word_analogy(word_a: str, word_b: str, word_c: str, top_n: int = 5) -> str`

Solves word analogies: A is to B as C is to ?

**Parameters:**
- `word_a` (str): First word (A)
- `word_b` (str): Second word (B)
- `word_c` (str): Third word (C)
- `top_n` (int): Number of results (1-10)

**Returns:**
- `str`: Formatted markdown with analogy results

**Example:**
```python
from app.services import word_analogy

result = word_analogy("tatÄƒl", "fiul", "mama", top_n=5)
print(result)
```

---

#### `get_model_info() -> dict`

Returns model and dataset statistics.

**Returns:**
- `dict`: Dictionary with model metadata

**Example:**
```python
from app.services import get_model_info

info = get_model_info()
print(f"Vocabulary size: {info['w2v']['vocab_size']}")
print(f"Dataset size: {info['dataset_size']}")
```

---

## Technical Details

### Similarity Metrics

#### Cosine Similarity
$$\cos(\vec{u}, \vec{v}) = \frac{\vec{u} \cdot \vec{v}}{||\vec{u}|| \cdot ||\vec{v}||}$$

**Range:** [0, 1] (for normalized vectors)
- 1.0: Identical direction (maximum similarity)
- 0.5: 60Â° angle
- 0.0: Orthogonal (minimum similarity)

#### Euclidean Distance
$$d(\vec{u}, \vec{v}) = \sqrt{\sum_{i=1}^{n} (u_i - v_i)^2}$$

#### Vector Norm (L2)
$$||\vec{v}|| = \sqrt{\sum_{i=1}^{n} v_i^2}$$

### Model Parameters Summary

#### Word2Vec (CBOW)
| Parameter | Value | Impact |
|-----------|-------|--------|
| vector_size | 100 | Dimensionality (higher = more expressive but slower) |
| window | 5 | Context size (wider window = more contextual info) |
| min_count | 2 | Frequency threshold (filters rare words) |
| epochs | 10 | Training passes (more = better but slower) |
| sg | 0 | CBOW mode (1 = Skip-gram) |

#### Doc2Vec (PV-DM)
| Parameter | Value | Impact |
|-----------|-------|--------|
| vector_size | 100 | Dimensionality |
| window | 5 | Context size |
| min_count | 2 | Frequency threshold |
| dm | 1 | PV-DM mode (0 = DBOW) |
| epochs | 20 | Training passes (higher for document-level) |

### Performance Considerations

**Model Size:**
- Word2Vec: ~34 MB
- Doc2Vec: ~24 MB
- Total: ~58 MB

**Memory Usage:**
- spaCy model: ~50-100 MB
- Loaded vectors in memory: ~20-50 MB
- Application overhead: ~100-200 MB
- **Total:** ~200-400 MB

**Query Performance:**
- Word similarity lookup: < 10ms
- Word pair comparison: < 20ms
- Sentence similarity inference: 50-200ms
- 10 similar documents: 100-300ms

**Optimization Tips:**
1. Use lemmatization for better matches
2. Avoid very short queries (< 2 tokens)
3. Query words must exist in vocabulary
4. Doc2Vec inference slower than Word2Vec lookup

### Common Issues & Solutions

#### Issue: "Word not found in vocabulary"
**Cause:** Query word doesn't exist in training data
**Solution:** Use related words or check spelling

#### Issue: "Doc2Vec model not available"
**Cause:** Model loading failed due to version mismatch
**Solution:** Reinstall gensim/numpy or retrain models

#### Issue: Slow inference on sentences
**Cause:** Long sentences require more processing
**Solution:** Use shorter, more focused queries

### File Formats

#### Model Files
- `.model`: Gensim binary format (Word2Vec/Doc2Vec)
- `.model.dv.vectors.npy`: NumPy vector array

#### Code Structure
- `.py`: Python source files
- `.ipynb`: Jupyter notebook (training code)

---

## Future Enhancements

1. **Multilingual Support:** Expand to English, French, German
2. **Advanced Visualizations:** t-SNE/UMAP plots of embeddings
3. **Similarity Matrices:** Visualize word relationships
4. **API Endpoint:** REST API for programmatic access
5. **Caching:** Speed up repeated queries
6. **Fine-tuning:** Allow user-guided model updates
7. **Custom Datasets:** Train on different corpora

---

## References

- [Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)
- [Gensim Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)
- [spaCy Romanian Model](https://spacy.io/models/ro)
- [Gradio Documentation](https://gradio.app/)
- [Word2Vec Paper](https://arxiv.org/abs/1301.3781)
- [Doc2Vec Paper](https://arxiv.org/abs/1405.4053)

---

## License

This project is for educational purposes.

---

## Contact & Support

For issues or questions:
1. Check the troubleshooting section
2. Review model loading errors in console
3. Verify all dependencies are installed
4. Ensure models are in the project root directory

---

**Last Updated:** January 2026
**Version:** 1.0
